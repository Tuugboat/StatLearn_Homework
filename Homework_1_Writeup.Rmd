---
title: "Homework 1 Writeup"
author: "Robert Petit"
date: "2/5/2022"
output: 
  md_document : default
  pdf_document : default
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, output_format = "all") })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

```{r Library Decleration and Data Importing, echo=FALSE}
#Libraries used for various plots
library(tidyverse) #for wrangling
library(mosaic) #for wrangling
library(ggplot2) #for plotting
library(ggthemes) #to make things pretty
library(ggmap) #for the map
library(ggrepel) #to format labels on plots
library(caret)
library(modelr)
library(foreach)
library(rsample)

ABIA = read.csv("ABIA.csv")
AirportCodes = read.csv("airport-codes.csv")
Billboard = read.csv("billboard.csv")
Sclass = read.csv("sclass.csv")
Olympics = read.csv("olympics_top20.csv")

USMap = readRDS(file="USMap.RDS")
#This is loaded from an RDS because in order to generate it normally requires registering an API key for GoogleMaps then downloading it. 
#Generally, I want to avoid RDS as a way to do things, but in this case it just made things much simpler

#Color sets of differing lengths for use later
ColorSet10 = c("#88CCEE","#CC6677","#DDCC77","#117733","#332288",
            "#AA4499","#44AA99","#999933","#882255","#661100")
ColorSet19 = c("#7F3C8D","#11A579","#3969AC","#F2B701","#E73F74","#80BA5A",
"#E68310","#008695","#CF1C90","#f97b72","#4b4b8f","#A5AA99",
"#88CCEE","#CC6677","#DDCC77","#117733","#332288","#AA4499","#44AA99")
```

# Question 1
## Key Question
The key question here is twofold: first, do flights across the US tend to be delayed by distance or on an airport-by-airport basis. Based on that result, we also want an idea of what the best day of the week is to fly.

## Methods and Figures Pt. 1
The answer to the first questions becomes fairly obvious when checking two different graphs. First, we can simply plot the distance and expected delays from incoming and outgoing flights
```{r Plot Lable Setup, echo=FALSE}
#Presetting some captions and titles for cleaner code in the writeup
OutDistPlot_Labs = labs(title = "Flight Delays By Distance: Outbound")
InDistPlot_Labs = labs(title = "Flight Delays by Distance: Inbound")


Inbound_Title = "Average Arrival Delay by Airport: Inbound"
Inbound_Caption = "Larger points correspond to larger delays/gains. 
  Red dots are delays, blue dots are early arrivals. Note the lack of blue dots"
InboundMap_Labs = labs(title = Inbound_Title, caption = Inbound_Caption)


Outbound_Title = "Average Departure Delay by Airport: Outbound"
Outbound_Caption = "Larger points correspond to larger delays/gains. 
  Red dots are delays, blue dots are early departures Note the lack of blue dots"
OutboundMap_Labs = labs(title = Outbound_Title, caption = Outbound_Caption)

```


```{r Basic Plots, echo=FALSE}
#Step one, a scatter plot of the distances
#Start by filtering our data into inbound and outbound sets by filtering on origin/destination
ABIA_In_ByFlight =  ABIA %>%
  filter(Dest == 'AUS')

ABIA_Out_ByFlight =  ABIA %>%
  filter(Origin == 'AUS')

In_DistPlot = ggplot(data=ABIA_In_ByFlight) +
  geom_point(mapping=aes(Distance, ArrDelay)) +
  ylim(0, 900) +
  InDistPlot_Labs
In_DistPlot

Out_DistPlot = ggplot(data=ABIA_Out_ByFlight) +
  geom_point(mapping=aes(Distance, DepDelay)) +
  ylim(0, 900) +
  OutDistPlot_Labs
Out_DistPlot

```

To do an eyeball test of the airport-dependency of delays, we can map the delays to the US
```{r Airport Wrangling, echo=FALSE}
AirportCodes_Filtered = AirportCodes %>%
  #Our one big filter, this gives us only those airports in the US
  #Who have valid IATA codes and are still open (closed airports overlap codes with open ones in some cases)
  filter(iata_code != "", iso_country == "US", type != "closed") %>% #First task
  #Separate region into country/state (country dropped later)
  separate(iso_region, c("Country", "State"), "-", remove=TRUE) %>%
  #Separate coords into long/lat for ggmap to use later
  separate(coordinates, c("Long", "Lat"), ", ") %>%
  mutate(Name = name, Code = iata_code, #renaming these columns for styling
         Long = as.numeric(Long), Lat = as.numeric(Lat)) %>% 
  select(Name, Code, State, Long, Lat) %>%
  arrange(Code)

#Generating ABIA_In_ByPort by filtering on destinations that are Austin
#After grouping by the origin, we generate Count and average delay times
#Following that, we merge the airport codes with their long/lat
#We drop airports with fewer than 100 flights just to keep things reasonable
#And get rid of a some missing data errors
#We then merge it with the airport codes from above
#This leaves us with a list of all airports that fly into Austin, the number of flights they send, their mean delays
#and a Long/Lat for each that we can put on a map
ABIA_In_ByPort =  ABIA %>%
  filter(Dest == 'AUS') %>%
  group_by(Origin) %>%
  summarize(Count = n(), MeanArrDelay = mean(ArrDelay, na.rm=TRUE)) %>% #na.rm == TRUE covers some missing data points here. This is used consistently throughout the rest of the code.
  filter(Count>=100) %>%
  arrange(desc(MeanArrDelay)) %>%
  merge(AirportCodes_Filtered, by.x="Origin", by.y="Code")

#We generate some info about the destinations in the same way that we do _In
#The only differences is that Origin == Austin and arrivals become departures
ABIA_Out_ByPort = ABIA %>%
  filter(Origin == 'AUS') %>%
  group_by(Dest) %>%
  summarize(Count = n(), MeanDepDelay = mean(DepDelay, na.rm=TRUE)) %>%
  filter(Count>=100) %>%
  arrange(desc(MeanDepDelay)) %>%
  merge(AirportCodes_Filtered, by.x="Dest", by.y="Code")


#Plotting the in and outbound maps
#color based on positive or negative delays (positive is bad)
#Size is based on average time (scaled for readability)
Inbound_MapPlot = ggmap(USMap) +
  geom_point(aes(x=Lat, y=Long),
             color=ifelse(ABIA_In_ByPort$MeanArrDelay>0, yes="red", no="blue"),
             size=abs(ABIA_In_ByPort$MeanArrDelay/3), data=ABIA_In_ByPort) +
  InboundMap_Labs +
  xlab("Longitude") + ylab("Lattitude")
Inbound_MapPlot

Outbound_MapPlot = ggmap(USMap) +
  geom_point(aes(x=Lat, y=Long),
             color=ifelse(ABIA_Out_ByPort$MeanDepDelay>0, yes="red", no="blue"),
             size=abs(ABIA_Out_ByPort$MeanDepDelay/3), data=ABIA_Out_ByPort) +
  labs(title = Outbound_Title, caption = Outbound_Caption) +
  xlab("Longitude") + ylab("Lattitude")
Outbound_MapPlot

```

## Results Pt. 1
This is a pretty clear indication that the delays based are on a per-airport basis and not a flight-distance basis. There are certainly more formal regressions we could run to examine their significance, but for our purposes here, an eyeball test is more than adequate, if only because this is extremely intuitive and the purpose of this question was more to graph things on a map. For more interesting analysis, we can jump to our second question: Which days are best for each of the largest airports.

## Methods and Figures Pt. 2
```` {r Map Lable Setups, echo=FALSE}
#Again, right at the top we have aesthetics for some plots that don't matter untiil the end
#This is just for clean reading top-bottom of our code

#Setting up labels for days of the week to use in plots
Day_Labs = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")
names(Day_Labs) = c(1, 2, 4, 5, 6, 7)


#Graph titles and captions
InByDay_Title = "Mean Arrival Delay by Day"
InByDay_Caption = "Mean arrival delay by day\
Seperated by origin airport and day of the week"
InByDay_Labs = labs(title = InByDay_Title, caption = InByDay_Caption)

OutByDay_Title = "Mean Departure Delay by Day"
OutByDay_Caption = "Mean departure delay\
Seperated by destination and day of the week"  
OutByDay_Labs = labs(title = OutByDay_Title, caption = OutByDay_Caption)

````

```{r Best Day Charts, echo=FALSE}
#Grab a vector for the top 10 most frequented airports in our dataset
#Filtering to In/Out correctly, then counting, arranging, listing
#head(10) does a good job of simply grabbing the top. pull() gets the values as a vector so that we can use %in% later
ABIA_In_Top10Filter = ABIA %>%
  filter(Dest == 'AUS') %>%
  group_by(Origin) %>%
  summarize(Count = n()) %>%
  arrange(desc(Count)) %>%
  head(10) %>%
  pull(Origin)

ABIA_Out_Top10Filter = ABIA %>%
  filter(Origin == 'AUS') %>%
  group_by(Dest) %>%
  summarize(Count = n()) %>%
  arrange(desc(Count)) %>%
  head(10) %>%
  pull(Dest)

#This is pretty simple, just grouping each one by day/week and then summarizing
#We filter by which ones are in the list of top10
#the flight number and arrival delay. Arrange is superfluous here I think
#But useful for checking data manually
ABIA_In_ByDay =  ABIA %>%
  filter(Dest == 'AUS', Origin %in% ABIA_In_Top10Filter) %>%
  group_by(Origin, DayOfWeek) %>%
  summarize(Count = n(), MeanArrDelay = mean(ArrDelay, na.rm=TRUE)) %>%
  arrange(desc(DayOfWeek))

ABIA_Out_ByDay = ABIA %>%
  filter(Origin == 'AUS', Dest %in% ABIA_Out_Top10Filter) %>%
  group_by(Dest, DayOfWeek) %>%
  summarize(Count=n(), MeanDepDelay = mean(DepDelay, na.rm=TRUE)) %>%
  arrange(desc(DayOfWeek))

#Finally, to the plots. Straightforward facetted graphs here, but with some themes and manual coloring to make them look nicer.
#We also manually set the y-scale so that they match.
#If you are wondering where ColorSet and _Labs are set, it is above in the un-included section
#The X axis text is stripped here because in PDF view it is impossible to make it not cluttered.
#Thankfully, color coord comes in handy in the legend
InByDay_Plot = ggplot(data = ABIA_In_ByDay) +
  geom_col(mapping = aes(Origin, MeanArrDelay, fill=Origin)) +
  scale_fill_manual(values = ColorSet10) +
  theme_minimal() +
  facet_wrap(~DayOfWeek, nrow=5, labeller=labeller(DayOfWeek = Day_Labs)) +
  ylim(0, 30) +
  InByDay_Labs +
  theme(axis.text.x = element_blank())
InByDay_Plot

OutByDay_Plot = ggplot(data = ABIA_Out_ByDay) +
  geom_col(mapping = aes(Dest, MeanDepDelay, fill=Dest)) +
  scale_fill_manual(values = ColorSet10) +
  theme_minimal() +
  facet_wrap(~DayOfWeek, nrow=5, labeller=labeller(DayOfWeek = Day_Labs))+
  ylim(0, 30) +
  OutByDay_Labs +
  theme(axis.text.x = element_blank())
OutByDay_Plot  
```
And, in addition to the plots, a we can manually find the best day for each:
```{r Best Day tables, echo=FALSE}
#Top 10 origin airports and their least-delayed day on average, starting with 1=Monday")
#-------------------------------------------------------------------------------------")
ABIA_In_ByDay %>% group_by(Origin) %>% slice_min(MeanArrDelay, n=1) %>% arrange(desc(Count)) %>% head(10)
#Top 10 destination airports and their least-delayed day on average, starting with 1=Monday")
#-------------------------------------------------------------------------------------")
ABIA_Out_ByDay %>% group_by(Dest) %>% slice_min(MeanDepDelay, n=1) %>% arrange(desc(Count)) %>% head(10)
```

## Results Pt. 2
From the above, we can clearly see the best days to fly into or out of each of these airports. DFW, which is the most trafficked airport for Austin, is best flown from and to on a Saturday whereas Pheonix should be flown from on Saturday, but flown to on a Monday. 

# Question 2
## Part A
A table of the top 10 songs, measured by weeks in the top 100 chart
```{r Quick BB Table, echo=FALSE}
#Generating a chart of the top 10 songs since 1958, by week on the chart
Billboard %>% 
  group_by(song, performer) %>% 
  summarize(Count = n()) %>% 
  arrange(desc(Count)) %>% 
  head(10)
```

## Part B
Diversity by year, measured by the number of unique songs on the in the top 100 each year
```{r Song Diversity Plots, echo=FALSE}
Billboard_YearDiversity = Billboard %>%
  filter(year != 1958, year != 2021) %>% #drop end years
  select(song, year) %>% #isolate the only columns we care about so unique() is easy
  unique() %>% #delete duplicates
  group_by(year) %>% #collapse to years
  summarize(UniqueSongs = n()) %>% #count the occurrences
  #Adding some variables to use in coloring later; this sets a variable call MaxMin to 1 for the minimum, 2 for the maximum, 0 otherwise
  mutate(Bound = ifelse(min(UniqueSongs) == UniqueSongs | (max(UniqueSongs) == UniqueSongs), 1, 0),
         MaxMin = ifelse(Bound == 1 & (max(UniqueSongs) == UniqueSongs), 2, Bound)) %>%
  select(!Bound) #Drop superfluous column

#This leaves us with a plot that has 2 important values: years and the number of unique songs on their chart
#There is also a column called MaxMin with the value 1 for the minimum number, 2 for the maximum, and 0 otherwise
#By using discrete integers, we can filter with > easily later but still use factor(MaxMin) to get it categorically



#Below, we are going to use the geom_text_repel library to get our text well placed.
#Some of this is weird and more than a bit overkill
#The hardest part to read is the nudge_y, where we have to reconstruct our subset because for some reason we cannot
#Refer to the variables in the table within the ifelse. This is annoying, but not impossible to deal with
Billboard_DiversityPlot = ggplot(data = Billboard_YearDiversity, 
                                 aes(year, UniqueSongs, label=year)) +
  geom_line() +
  #below this line, we are no longer dealing with requirements for homework, just making things pretty
  geom_point(aes(color=factor(MaxMin))) + #Set the color based on the categorical version of the discrete
  ylim(300, 900) + #Make the plot larger than required to make room for labels
  geom_text_repel(data=subset(Billboard_YearDiversity, MaxMin>0), aes(color=factor(MaxMin)), 
                  point.padding = 0.5, 
                  nudge_y = ifelse(subset(Billboard_YearDiversity, MaxMin>0)$UniqueSongs>600, 10, -10)) +
  scale_color_manual(values = c("black", "red", "green")) +
  theme_minimal() +
  theme(legend.position = "none") + #delete the legend because it is unnecessary
  ggtitle(label = "Number of Unique Songs per Year") + 
  xlab("Year") + 
  ylab("Unique Songs")
Billboard_DiversityPlot
```

## Part C
A list of artists with more than 30 songs that spent at least ten weeks in the top 100, listed by the number of 10-week hits
```{r TenWeek Plots, echo=FALSE}
#Wrangle in two stages
Billboard_ByTenWeek = Billboard %>%
  group_by(song, performer) %>% #step 1, filter out songs that spent fewer than 10 weeks on the chart
  summarize(WeeksOnChart = n()) %>%
  filter(WeeksOnChart >= 10) %>%
  group_by(performer) %>% #Step 2, filter out artists with fewer than 30 songs in the remaining list
  summarize(TWHNumber = n()) %>%
  filter(TWHNumber >= 30)

#Setting up our plot to be ordered, highest to lowest, by number of ten-week hits
Billboard_TenWeekPlot = ggplot(data = Billboard_ByTenWeek, 
                               aes(fct_reorder(performer, TWHNumber), TWHNumber, fill=performer)) +
  geom_col() +
  #Again, after this line we just want things looking good
  geom_text(aes(label=TWHNumber, color=performer), nudge_y = 1.5) +
  scale_fill_manual(values=ColorSet19) +
  scale_color_manual(values=ColorSet19) +
  theme_minimal() +
  theme(legend.position = "none") + #delete the legend because it is unnecessary
  ggtitle("Artists with Ten-Week Hits")+
  xlab("Artist") +
  ylab("Number of Ten-Week Hits") +
  coord_flip()

Billboard_TenWeekPlot
```

# Question 3
## Part A
This question is written in a way that has me confused. I am unsure if we are supposed to get *each* events' 95th percentile or simply for Athletics' events as a whole. In the case of each event individually, it is given by the following table.
``` {r Height Percentile per Event, echo=FALSE}
Olympics_95thPercentile = Olympics %>%
  filter(sex=="F", sport=="Athletics") %>% #Filter into female
  group_by(event) %>% #Group by each event
  arrange(height) %>% #arrange stacks by height
  filter(row_number()>=floor((n()*0.95))) %>% #drop bottom 95%
  summarize(p0.95 = max(height)) #Grab the minimum of the remaining 5%
head(Olympics_95thPercentile, 27)
```
However, if we are looking simply for the 95th percentile of all Athletics medalists, it is produced in the following (much smaller) table
```{r Height Percentile Overall, echo=FALSE}
Olympics_Athletics = Olympics %>%
  filter(sex=="F", sport=="Athletics") #Filter into female
quantile(Olympics_Athletics$height, 0.95)
```
## Part B
The top variation among all events in females competitors heights is given by
``` {r, echo=FALSE}
Olympics_Height_SD = Olympics %>%
  filter(sex=="F") %>%
  #To reduce to only Athletics events:
  #filter(sport == "Athletics") %>%
  group_by(event) %>%
  summarize(Std_Dv = sd(height)) %>%
  arrange(desc(Std_Dv))
head(Olympics_Height_SD, 1)
```
## Part C
```{r Olympian Age over time, echo=FALSE}
Olympics_Age_ByYear = Olympics %>%
  filter(sport=="Swimming") %>%
  group_by(sex, year) %>%
  summarize(Age_Avg = mean(age), Gender = (ifelse(sex=="M", "Male", "Female"))) #Extra variable is added here as a workaround
#For some reason, the label in the below plot was acting up so I simple created variables styled
#According to the desired formatting rules

Olympics_AgePlot = ggplot() +
  geom_line(aes(year, Age_Avg, color=Gender), data=subset(Olympics_Age_ByYear, sex=="F")) +
  geom_point(aes(year, Age_Avg, color=Gender), data=subset(Olympics_Age_ByYear, sex=="F")) +
  geom_line(aes(year, Age_Avg, color=Gender), data=subset(Olympics_Age_ByYear, sex=="M")) +
  geom_point(aes(year, Age_Avg, color=Gender), data=subset(Olympics_Age_ByYear, sex=="M")) +
  scale_color_manual(values=c("purple", "#009107")) +
  theme_minimal() +
  labs(title="Average Age of Swimming Medalists by Year", caption="With the exception of a few notable years in the 1910s and 1920s,\
       age for both men and women have been steadily increasing") +
  ylab("Average Age") +
  xlab("Year")
Olympics_AgePlot
```


# Question 4

``` {r Function Decleration, echo=FALSE}
KNN_MultiKTesting <- function(DataSet, K_Grid, labs_KPlot, labs_PredPlot, SplitProp = 0.9) {
  
  #Step 1, Split data and run KNN models
  DataSet_Split = initial_split(DataSet, prop=SplitProp)
  DataSet_Train = training(DataSet_Split)
  DataSet_Test = testing(DataSet_Split)
  #Simple version, for each candidate K, train the model and then output the K and associated RMSE as a list
  K_Errors = foreach(CurrentK=K_Grid, .combine='rbind') %do% {
    CurrentModel = knnreg(price ~ mileage, k=CurrentK, data = DataSet_Train, use.all=FALSE)
    c(K=CurrentK, Error = modelr::rmse(CurrentModel, data=DataSet_Test))
  } %>% as.data.frame
 
  #Isolate the minimum K for use in later graphs/parameters
  K_Errors = K_Errors %>%
    mutate(Min = (Error == min(Error)))
  
  #First plot of import, graphing the RMSE for various KNN models
  Plot_K = ggplot(data = K_Errors, aes(K, Error, label=K)) +
    geom_line() +
    geom_point(aes(color=Min)) +
    #Adding some labels and theme stuff
    geom_text_repel(data=subset(K_Errors, Min), aes(color=Min), 
                    nudge_y = -10) +
    scale_color_manual(values=c("black", "green")) +
    theme_minimal() +
    labs_KPlot +
    theme(legend.position = "none")
  
  #Pull out the minimum K as a num
  MinError = K_Errors %>%
    filter(Min) %>%
    pull(K)
  
  #Rebuild the model for the minimum K. I could probably do this with more memory efficiency in the
  #Original for loop, but that's a hassle and it's not expensive here
  MinError_Model = knnreg(price ~ mileage, data=DataSet_Train, k=MinError)
  
  #Add predictions for the test case
  DataSet_Test = DataSet_Test %>%
    mutate(KNNPred = predict(MinError_Model, DataSet_Test))
  
  #Setting plot of import, the data set with the prediction overlay
  Plot_Prediction = ggplot() +
    geom_point(aes(mileage, price), data=DataSet) +
    geom_line(aes(mileage, KNNPred), color="red", size=1.5, data=DataSet_Test)+
    #Theme stuff
    theme_minimal() +
    labs_PredPlot +
    xlab("Mileage") + 
    ylab("Price")
  
  #Building a return vector that is named for clarity
  ReturnVector = list(Plot_K, Plot_Prediction, MinError)
  names(ReturnVector) <- c("Kplot", "PredictionPlot", "KVal")
  return(ReturnVector)
  
}

```

RMSE values for a collection of K values with a 90/10 test-train split. Based on data for the S-Class 350
``` {r 350 KPlot, echo=FALSE}
K_Candidates = c(2, 4, 6, 8, 10, 15, 20, 25, 30, 35, 40, 45,
                 50, 60, 70, 80, 90, 100, 125, 150, 175, 200, 250, 300)

Sclass_350 = Sclass %>%
  filter(trim=="350") %>%
  select(price, mileage)

Labs_350_KPlot = labs(title="K vs. RMSE for S-Class 350 with 90/10 Split")
Labs_350_PredPlot = labs(title="Price vs. Mileage, S-Class 350", caption="KNN Prediction overlaid")

Sclass_350_Plots = KNN_MultiKTesting(Sclass_350, K_Candidates, Labs_350_KPlot, Labs_350_PredPlot)

Sclass_350_Plots["Kplot"]
```

The best-case K, in this case `Sclass_350_Plots["KVal"]`, overlaid on the observed data
```{r 350 PredPlot, echo=FALSE}
print("For K of ")
Sclass_350_Plots["KVal"]
Sclass_350_Plots["PredictionPlot"]

```

For the S-Class 65AMG, we can see the errors of each of the same K values as above.
```{r 65AMG KPlot, echo=FALSE}

Sclass_65AMG = Sclass %>%
  filter(trim=="65 AMG") %>%
  select(price, mileage)

Labs_65AMG_KPlot = labs(title="K vs. RMSE for S-Class 65AMG with 90/10 Split")
Labs_65AMG_PredPlot = labs(title="Price vs. Mileage, S-Class 65AMG", caption="KNN Prediction overlaid")

Sclass_65AMG_Plots = KNN_MultiKTesting(Sclass_65AMG, K_Candidates, Labs_65AMG_KPlot, Labs_65AMG_PredPlot)

Sclass_65AMG_Plots["Kplot"]
```

And again, with a best-case K, we can see how it fits the data
``` {r 65AMG PredPlot, echo=FALSE}
print("For K of ")
Sclass_65AMG_Plots["KVal"]
Sclass_65AMG_Plots["PredictionPlot"]

```

It is worth noting that, in this case, the above values are not at all stable. Below, we can see the variation across a collection of tests on the S-Class 350 data.
```{r K Variation Plots, echo=FALSE}

MinErrorKCollection = foreach(i=1:50, .combine='rbind') %do% {
  c(Kval = KNN_MultiKTesting(Sclass_350, K_Candidates, Labs_350_KPlot, Labs_350_PredPlot)$KVal, Iterr = i)
} %>% as.data.frame

ExcessPlot = ggplot(data=MinErrorKCollection, aes(Iterr, Kval)) +
  geom_line() +
  geom_point() +
  labs(title="Varying optimal K Values Across Multiple Test/Train Splits") +
  ylab("K Value") +
  xlab("Iteration Number") +
  theme_minimal()
ExcessPlot
```